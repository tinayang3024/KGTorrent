{"cells":[{"metadata":{"_cell_guid":"d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1","_uuid":"9dacb7a2a5a9e582d7e597be4ea479cedd79489a"},"cell_type":"markdown","source":"# Identifying authors - Who wrote that?\nStarted on 30 Oct 2017\n\nThis notebook is inspired by:\n* Machine Learning: Classification - Coursera course by University of Washington,\nhttps://www.coursera.org/learn/ml-classification\n* Machine Learning with Text in scikit-learn - Kevin Markham's tutorial at Pycon 2016, \nhttps://m.youtube.com/watch?t=185s&v=ZiKMIuYidY0\n* Kernel by bshivanni - \"Predict the author of the story\", \nhttps://www.kaggle.com/bsivavenu/predict-the-author-of-the-story\n* Kernel by SRK - \"Simple Engg Feature Notebook - Spooky Author\",\nhttps://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author"},{"metadata":{"_cell_guid":"06e8c4a5-feea-41ea-8831-13ab7089dcc5","_uuid":"8f9dbcf8df12e40c0daee0f48d91fd07cd084cae"},"cell_type":"markdown","source":"Comments:\n\n* In this kernel, I did a weighted averaging of the 'proba' of the 2 models to see the performance.\n* I added character counts as features to the sparse matrix to see if prediction performance will improve.\n"},{"metadata":{"_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b","_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b"},"cell_type":"markdown","source":"## Read \"train.csv\" and \"test.csv into pandas"},{"metadata":{"_cell_guid":"094fff47-db10-447c-965e-08056f718bde","_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","collapsed":true,"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa","_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6"},"cell_type":"markdown","source":"## Examine the train data"},{"metadata":{"_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654","_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","collapsed":true,"trusted":true},"cell_type":"code","source":"# check the class distribution for the author label in train_df?\ntrain_df['author'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"391dfeb1-5549-4f85-ac4c-4ed66e9cce6b","_uuid":"d2bd8ddc6445aad024d327d7b78004f80cfd83f6"},"cell_type":"markdown","source":"#### The class distribution looks balanced."},{"metadata":{"_cell_guid":"b26588a7-9a7f-4183-98b2-fb94a70bedaa","_uuid":"1e97432af65b6b75b436daabc83bdf57775a59c1","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the character length for the rows and record these\ntrain_df['text_length'] = train_df['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5ac5111-5bba-44c9-a039-7bb01a5bfd59","_uuid":"448c3492fc2fe24f30bd7b97047d69f16b58ca2f","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at the histogram plot for text length\ntrain_df.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25","_uuid":"d28a801b4057518f87e20c46a77f9dc8757ab944"},"cell_type":"markdown","source":"#### Most of the text length are 500 characters and less. Let's look at the summary statistics of the text lengths by author."},{"metadata":{"_cell_guid":"5da0987d-d871-4f53-b605-8f89ab4dfd94","_uuid":"d5d5bf3219d1a53fae0bfab69844e118cda32bab","collapsed":true,"trusted":true},"cell_type":"code","source":"EAP = train_df[train_df['author'] =='EAP']['text_length']\nEAP.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f17e03d-50dd-4aae-b579-a8f0d68ead5a","_uuid":"4600bd930b16cd5e576701c7baa4304c6b6b0836","collapsed":true,"trusted":true},"cell_type":"code","source":"EAP.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9d8733d-c134-4f9b-8184-26492b995602","_uuid":"a3e02aec85a3817124145d6c4ca602b9e0a73223","collapsed":true,"trusted":true},"cell_type":"code","source":"MWS = train_df[train_df['author'] == 'MWS']['text_length']\nMWS.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4befa810-7786-451d-b476-b41dc5036869","_uuid":"487062ed9cca7724c86631174501c4bf2c2b90ad","collapsed":true,"trusted":true},"cell_type":"code","source":"MWS.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73db4ab6-3395-4972-be02-36d4bc393899","_uuid":"e3be2b92e83f2a4336bd20677395930597fea6c6","collapsed":true,"trusted":true},"cell_type":"code","source":"HPL = train_df[train_df['author'] == 'HPL']['text_length']\nHPL.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f58ad06-6ce6-4921-bcbf-536cc7c009b3","_uuid":"f8cf435292bd01efe234667d55239a8733b60950","collapsed":true,"trusted":true},"cell_type":"code","source":"HPL.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7c25d701-f75d-4902-b51d-6ed46cdb27a8","_uuid":"71efe8c7cf1fa93eec845046b031852fcc4ed8ac"},"cell_type":"markdown","source":"## Similarly examine the text length & distribution in test data"},{"metadata":{"_cell_guid":"0993d06b-495e-428a-933a-6ffec6bdcef3","_uuid":"36e4d00a5afc15e6b04ffa1e79421396d051f614","collapsed":true,"trusted":true},"cell_type":"code","source":"# examine the text characters length in test_df and record these\ntest_df['text_length'] = test_df['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"828b9990-d78e-46c7-b011-1c66e6e6be79","_uuid":"1cc05875b88e54b5a1079eafc088207536dea2f3","collapsed":true,"trusted":true},"cell_type":"code","source":"test_df.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2bbf246-f098-425a-99e3-2eb2126b975c","_uuid":"db69eb685cd1be44de2224c399cbdeabb5ceaa06"},"cell_type":"markdown","source":"#### The proportion of text which are long in the test data is very similar to that in the train data."},{"metadata":{"_cell_guid":"3c3f238d-1b6a-48a0-bcee-40031e4a2536","_uuid":"d0709c2d7c606f27b7b1abeeed821aaeee00fb6e"},"cell_type":"markdown","source":"## Some preprocessing of the target variable to facilitate modelling"},{"metadata":{"_cell_guid":"1a1a7fb8-9dfc-47c9-9114-f4708d109854","_uuid":"46438c851ef26de2bd7a3736c13abeb938519800","collapsed":true,"trusted":true},"cell_type":"code","source":"# convert author labels into numerical variables\ntrain_df['author_num'] = train_df.author.map({'EAP':0, 'HPL':1, 'MWS':2})\n# Check conversion for first 5 rows\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58afa26f-1ac7-4231-898f-882c2fcfff78","_uuid":"45a31d197362bff5f9a97b5308c67644cfd3b58e"},"cell_type":"markdown","source":"#### Let's limit all text length to 700 characters for both train and test (for less outliers in data)."},{"metadata":{"_cell_guid":"9adbff66-3f6b-44d5-8221-f5c610a0a849","_uuid":"cef4b909e7b6dce67a27e726ced38c0b3a308ece","collapsed":true,"trusted":true},"cell_type":"code","source":"train_df = train_df.rename(columns={'text':'original_text'})\ntrain_df['text'] = train_df['original_text'].str[:700]\ntrain_df['text_length'] = train_df['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dea487f2-01e5-4e32-b3aa-cc78b46cc4fe","_uuid":"7042d13445d5f4c205f67a315cea14f193cf9db5","collapsed":true,"trusted":true},"cell_type":"code","source":"test_df = test_df.rename(columns={'text':'original_text'})\ntest_df['text'] = test_df['original_text'].str[:700]\ntest_df['text_length'] = test_df['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f5abe72a-13a7-41f6-ae8e-1c34dca97110","_uuid":"cae81f6b1d9bb475fc486d5fbb81981025cc3672"},"cell_type":"markdown","source":"## Define X and y from train data for use in tokenization by Vectorizers"},{"metadata":{"_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b","_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","collapsed":true,"trusted":true},"cell_type":"code","source":"X = train_df['text']\ny = train_df['author_num']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a6e6a69-91b7-43a9-8e91-3c8fd20d2790","_uuid":"903c319c5b83198edf0af59d49818bd9071ec8dc"},"cell_type":"markdown","source":"## Split train data into a training and a test set"},{"metadata":{"_cell_guid":"438ceb39-2687-41b5-b7cb-8784a1aab35d","_uuid":"32e74067044fb018d0a71ed97c3326d578b1c0a6","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7010f557-ccce-4d8a-a289-d081d2c3c0c9","_uuid":"40e308981530947ddb7b68ac0075b36c7decde43","collapsed":true,"trusted":true},"cell_type":"code","source":"# examine the class distribution in y_train and y_test\nprint(y_train.value_counts(),'\\n', y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67","_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351"},"cell_type":"markdown","source":"## Vectorize the data using Vectorizer"},{"metadata":{"_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb","_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","collapsed":true,"trusted":true},"cell_type":"code","source":"# import and instantiate CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# vect = CountVectorizer()\n# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\nvect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\n# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\?|\\;|\\:|\\!|\\'')\nvect","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c","_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","collapsed":true,"trusted":true},"cell_type":"code","source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix created from X_train\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4","_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","collapsed":true,"trusted":true},"cell_type":"code","source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\nX_test_dtm = vect.transform(X_test)\n# examine the document-term matrix from X_test\nX_test_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94b6ac8d-c361-406f-b909-fa66073e0f17","_uuid":"db268b1314475c9ae3ce821762bc9c3dd2b0f059"},"cell_type":"markdown","source":"### Add character counts as a features to the sparse matrix using function `add_feature`"},{"metadata":{"_cell_guid":"f516368c-ed6f-4362-800e-1f65a2c2b527","_uuid":"352ea0d5f211cab206db0cf89812beeffdc4b233","collapsed":true,"trusted":true},"cell_type":"code","source":"def add_feature(X, feature_to_add):\n    '''\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    '''\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f15bb31-c93b-42bb-b4f4-a8e49dff8870","_uuid":"92c9ca1bae235ad67271a6f113634e70b4b8b06a","collapsed":true,"trusted":true},"cell_type":"code","source":"from string import punctuation\nX_train_chars = X_train.str.len()\nX_train_punc = X_train.apply(lambda x: len([c for c in str(x) if c in punctuation]))\nX_test_chars = X_test.str.len()\nX_test_punc = X_test.apply(lambda x: len([c for c in str(x) if c in punctuation]))\nX_train_dtm = add_feature(X_train_dtm, [X_train_chars, X_train_punc])\nX_test_dtm = add_feature(X_test_dtm, [X_test_chars, X_test_punc])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d051be8f-b902-42c9-af1f-fe2bb08cc697","_uuid":"f9ac5adf7318817090f8074f6a962bbb0b4f6551","collapsed":true,"trusted":true},"cell_type":"code","source":"X_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0eb8a63b-5e2a-446b-8f01-22ee05678b88","_uuid":"4383bf06b08fb4bcdbaa19165128e0044cfe04db","collapsed":true,"trusted":true},"cell_type":"code","source":"X_test_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"74e5e92e-9964-47bd-89fb-d39b62730954","_uuid":"bfd0cc30c28d8750c2b4fc395c00d9f3b186a4fe"},"cell_type":"markdown","source":"## Build and evaluate an author classification model using Multinomial Naive Bayes"},{"metadata":{"_cell_guid":"24e76bcf-3f0c-46b0-8370-e47ec1377b31","_uuid":"5498742f48ffcb39e09d347f5bf315fd882627c0","collapsed":true,"trusted":true},"cell_type":"code","source":"# import and instantiate the Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b04664ed-da58-4015-b5ef-515caed1a937","_uuid":"dbc9ea4f4ea0531152f22f4c7b57b7983b8518b0","collapsed":true,"trusted":true},"cell_type":"code","source":"# tune hyperparameter alpha = [0.01, 0.1, 1, 10, 100]\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'alpha':[0.01, 0.1, 1.0, 10.0, 100.0]}\ngrid_nb = GridSearchCV(nb, param_grid=grid_values, scoring='neg_log_loss')\ngrid_nb.fit(X_train_dtm, y_train)\ngrid_nb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b36a1718-972b-4f6d-baa6-fe9dd2402d5b","_uuid":"2b8a935a879c8605ed3ab246c387865c5ab8ba1b","collapsed":true,"trusted":true},"cell_type":"code","source":"# set with recommended hyperparameters\nnb = MultinomialNB(alpha=1.0)\n# train the model using X_train_dtm & y_train\nnb.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"521690d7-19e4-408e-b837-76582be3b408","_uuid":"20b7974ad1b6b3bd4c03d0110950410c491366e6","collapsed":true,"trusted":true},"cell_type":"code","source":"# make author (class) predictions for X_test_dtm\ny_pred_test = nb.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"022f4b59-ef5e-413e-b4ef-090d81ac2223","_uuid":"75c9bf8f66093512b8ca8275e88f38750c6d6128","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the accuracy of the predictions with y_test\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a538513-0860-48db-8626-5117499ea237","_uuid":"280a7405b142bba27d5a9b9cd2f778191158649b","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the accuracy of training data predictions\ny_pred_train = nb.predict(X_train_dtm)\nmetrics.accuracy_score(y_train, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d92d210-7255-4c33-ba42-bee0c32de1bb","_uuid":"be747bd888c6f48c9e1581c5341fa0eaba4b9753","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at the confusion matrix for y_test\nmetrics.confusion_matrix(y_test, y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fa8ac435-be40-48a6-8b46-b6e3860dbbee","_uuid":"3b87ae14e8f809a509207fee3c3e29925f19a2e4","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm\ny_pred_prob = nb.predict_proba(X_test_dtm)\ny_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48b380e8-5822-4376-aeea-295b229d62e9","_uuid":"9fed846b715fb77ee2549c63b02e3adce66b4f5f","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the log loss number\nmetrics.log_loss(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04ab10ee-4c43-41a2-87a8-ea3c3400a546","_uuid":"a989e534fe575ebeb3d5952e221c5b445631738a"},"cell_type":"markdown","source":"## Build and evaluate an author classification model using Logistic Regression"},{"metadata":{"_cell_guid":"e30be87f-e0a6-4fd7-bff2-b3c37737cbfe","_uuid":"e7e5707b19c35c8371a0cff7351bc8aadc33acd1","collapsed":true,"trusted":true},"cell_type":"code","source":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=8)\nlogreg","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc8c51b0-1187-471a-a4ee-957371d43104","_uuid":"e6cd7e92d4c27463effddedc8ce5fabb1358323d","collapsed":true,"trusted":true},"cell_type":"code","source":"# tune hyperparameter\ngrid_values = {'C':[0.01, 0.1, 1.0, 3.0, 5.0]}\ngrid_logreg = GridSearchCV(logreg, param_grid=grid_values, scoring='neg_log_loss')\ngrid_logreg.fit(X_train_dtm, y_train)\ngrid_logreg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fa49e9a-7a48-4aa7-b78a-bcd334b9332a","_uuid":"723423cc5420eac2c5771723ab31b2f4dad610c0","collapsed":true,"trusted":true},"cell_type":"code","source":"# set with recommended parameter\nlogreg = LogisticRegression(C=1.0, random_state=8)\n# train the model using X_train_dtm & y_train\nlogreg.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a298037-97bc-49eb-8cf7-bf52d4894f11","_uuid":"98f1dfbb75b68b65d49fa9f8fa69ebf64554e894","collapsed":true,"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_test = logreg.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9c459dae-783d-49e0-9b0c-dffd35419dbc","_uuid":"5000bb48f01b5632c5b3d4387154e32b6be4410a","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the accuracy of the predictions\nmetrics.accuracy_score(y_test, y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"816ea321-cdfd-41d0-b2f8-b89fcc513c43","_uuid":"847ef102dcb57e88f06003603a4ec7bd7cdde87f","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the accuracy of predictions with the training data\ny_pred_train = logreg.predict(X_train_dtm)\nmetrics.accuracy_score(y_train, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6f4a678-7072-44c4-a415-de7ba1d55d52","_uuid":"302632e4ecd6790e1cbb92baab53f98517028bcc","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at the confusion matrix for y_test\nmetrics.confusion_matrix(y_test, y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a238463-87eb-44c5-afcd-794accd04d71","_uuid":"c6a1543f17f4031e31b250aef5d98ddb6b1f1d0d","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the predicted probabilities for X_test_dtm\ny_pred_prob = logreg.predict_proba(X_test_dtm)\ny_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91e879ac-744b-4d10-b798-c99535e5c23b","_uuid":"ad9cfb160671a6fbcf92b8ca6e800440948cf232","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the log loss number\nmetrics.log_loss(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"962928a6-503e-4757-8518-b7b23f74d09a","_uuid":"ef4f2b854c0a2c346a6615ac8de40e5e4bcbe868"},"cell_type":"markdown","source":"## Train the Logistic Regression model with the entire dataset from \"train.csv\""},{"metadata":{"_cell_guid":"5018c932-1fc4-401b-8993-2f1a48f2415e","_uuid":"d7e09c9fe6524a9f964d50d8048c5226a4a67475","collapsed":true,"trusted":true},"cell_type":"code","source":"# Learn the vocabulary in the entire training data, and create the document-term matrix\nX_dtm = vect.fit_transform(X)\n# Examine the document-term matrix created from X_train\nX_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa9fdadf-9975-4a8d-a56d-a9c82fd52486","_uuid":"cd659e7f6a05b87ea52cf32edb3644020d323c9d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Add character counts features\nX_chars = X.str.len()\nX_punc = X.apply(lambda x: len([c for c in str(x) if c in punctuation]))\nX_dtm = add_feature(X_dtm, [X_chars, X_punc])\nX_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69cacde4-b731-40a8-b754-e3ecd4decff7","_uuid":"ab43f652017097166901c83c679485b444c6c192","collapsed":true,"trusted":true},"cell_type":"code","source":"# Train the Logistic Regression model using X_dtm & y\nlogreg.fit(X_dtm, y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48c91a38-87d7-4b72-a418-829730c9ee37","_uuid":"65f85a9921ce7d0664762fddb1b592a8bb005cae","collapsed":true,"trusted":true},"cell_type":"code","source":"# Compute the accuracy of training data predictions\ny_pred_train = logreg.predict(X_dtm)\nmetrics.accuracy_score(y, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8bc6812d-6592-412e-80a5-ae339488a0bf","_uuid":"968c394fe3ba44506a2a65606317b8d1983d24ee"},"cell_type":"markdown","source":"## Make predictions on the test data and compute the probabilities for submission"},{"metadata":{"_cell_guid":"4b22b19b-fbf5-429b-b980-fc954000dd1c","_uuid":"2e45db51b1b8d7ea951db649770e33cc725608eb","collapsed":true,"trusted":true},"cell_type":"code","source":"test = test_df['text']\n# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_dtm = vect.transform(test)\n# examine the document-term matrix from X_test\ntest_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fcde1e20-fabd-40a4-8e3f-25a97f78f8e2","_uuid":"046d2f98c331391af98f3ad4dcce20b6b95352c6","collapsed":true,"trusted":true},"cell_type":"code","source":"# Add character counts features\ntest_chars = test.str.len()\ntest_punc = test.str.count(r'\\W')\ntest_dtm = add_feature(test_dtm, [test_chars, test_punc])\ntest_dtm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19cc4c8b-ac52-4ed8-b7b3-9d0d1d96099d","_uuid":"3290ddf6fe948d9b4a0ed26419ef94e35f295e84","collapsed":true,"trusted":true},"cell_type":"code","source":"# make author (class) predictions for test_dtm\nLR_y_pred = logreg.predict(test_dtm)\nprint(LR_y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c051ec9-b17b-42b6-8449-6b692431d6c5","_uuid":"abaefe7c47cac96b429315d480900e8e9e800f98","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for test_dtm\nLR_y_pred_prob = logreg.predict_proba(test_dtm)\nLR_y_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73999e46-b1f2-47ed-8c13-2871dfbd8541","_uuid":"7ab689bd05df65f3a5e09fcefe7d80ba66615add"},"cell_type":"markdown","source":"## Train the Naive Bayes model with the entire dataset \"train.csv\""},{"metadata":{"_cell_guid":"7c9287c2-87f0-4344-8cf2-8cdf56c5e943","_uuid":"2bca382cdca372e9ba4b294b54336efcfb15c181","collapsed":true,"trusted":true},"cell_type":"code","source":"nb.fit(X_dtm, y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3e5dd33-a75e-44f7-b211-1cced31c033d","_uuid":"9a6a36102a4269e6e182375eeabcf4ce63ac94ee","collapsed":true,"trusted":true},"cell_type":"code","source":"# compute the accuracy of training data predictions\ny_pred_train = nb.predict(X_dtm)\nmetrics.accuracy_score(y, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"326c0751-6036-451b-8b23-fdec9ef4631a","_uuid":"7e9ed7c4b05fed9aafcec960e4414ddaaac852c3"},"cell_type":"markdown","source":"## Make predictions on test data"},{"metadata":{"_cell_guid":"fe35c157-77cf-48a3-9791-023725cd9784","_uuid":"6336049258e441b9507def0b9173570abd17e04c","collapsed":true,"trusted":true},"cell_type":"code","source":"# make author (class) predictions for test_dtm\nNB_y_pred = nb.predict(test_dtm)\nprint(NB_y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae1ce383-766a-4ef8-99cf-9df8f2bdfcf9","_uuid":"db1ff0ec20bcbdead790ace8ac5254775ca60e95","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculate predicted probablilities for test_dtm\nNB_y_pred_prob = nb.predict_proba(test_dtm)\nNB_y_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d0970eb-bc44-4fad-91b7-e2a23c2f986d","_uuid":"ee6d942b861235107e0d94174a8d21c2bad0e9ea"},"cell_type":"markdown","source":"## Create submission file\n#### Here I am combining the probabilities from the two models, using parameter alpha. "},{"metadata":{"_cell_guid":"4865057d-c375-4f3d-a8b3-fa36b18a3f7c","_uuid":"a2a24232c56e0d0567493401972c11df9fbdeec9","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"alpha = 0.6\ny_pred_prob = ((1-alpha)*LR_y_pred_prob + alpha*NB_y_pred_prob)\ny_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef99474c-ee91-40af-a4e0-7f86445e6841","_uuid":"ee067d4b61d323f1589970031ca121364e52a9f6","collapsed":true,"trusted":true},"cell_type":"code","source":"result = pd.DataFrame(y_pred_prob, columns=['EAP','HPL','MWS'])\nresult.insert(0, 'id', test_df['id'])\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bef9209e-c843-4ca6-be4f-35820ffa258d","_uuid":"86d7d2fa2b5671183438fe0769cfb7594eb4efa5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate submission file in csv format\nresult.to_csv('rhodium_submission_16.csv', index=False, float_format='%.20f')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a012382-ad72-4c76-b53c-1f756544e23e","_uuid":"cbbce014f73f54e9bf3e88e096ad66b89b88fcf4"},"cell_type":"markdown","source":"### Thank you for reading this.\n### Comments and tips are most welcomed.\n### Please upvote if you find it useful. Cheers!"},{"metadata":{"_cell_guid":"2b370cff-d22e-4393-94d5-98b4883a32d8","_uuid":"21a21a9ff3c174753c8d48495e058a1782bf7a76","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}