{"cells":[{"metadata":{"_cell_guid":"c67e806c-e0e6-415b-8cf0-ed92eba5ed37","_uuid":"34b6997bb115a11f47a7f54ce9d9052791b2e707"},"cell_type":"markdown","source":"# Overview\nThis is just a simple first attempt at a model using InceptionV3 as a basis and attempting to do regression directly on the age variable using low-resolution images (384x384) in attempt to match the winning solution [here](https://www.16bit.ai/blog/ml-and-future-of-radiology) which scored an ```mae_months``` on the test set of 4.2\n\nThis can be massively improved with \n* high-resolution images\n* better data sampling\n* ensuring there is no leaking between training and validation sets, ```sample(replace = True)``` is real dangerous\n* better target variable (age) normalization\n* pretrained models\n* attention/related techniques to focus on areas"},{"metadata":{"collapsed":true,"_cell_guid":"1b073998-389e-4972-9806-ff4297767a03","_uuid":"1f315ce4c0414edd872988eb5b4017a5c208eb8a","trusted":true},"cell_type":"code","source":"# copy the weights and configurations for the pre-trained models","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e94de3e7-de1f-4c18-ad1f-c8b686127340","_uuid":"c163e45042a69905855f7c04a65676e5aca4837b","trusted":true},"cell_type":"code","source":"!mkdir ~/.keras\n!mkdir ~/.keras/models\n!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n!cp ../input/keras-pretrained-models/resnet50* ~/.keras/models/","execution_count":2,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c3cc4285-bfa4-4612-ac5f-13d10678c09a","_uuid":"725d378daf5f836d4885d67240fc7955f113309d","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # showing and rendering figures\n# io related\nfrom skimage.io import imread\nimport os\nfrom glob import glob\n# not needed in Kaggle, but required in Jupyter\n%matplotlib inline ","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"c4b38df6-ffa1-4847-b605-511e72b68231","_uuid":"346da81db6ee7a34af8da8af245b42e681f2ba48","trusted":true},"cell_type":"code","source":"base_bone_dir = os.path.join('..', 'input', 'rsna-bone-age')\nage_df = pd.read_csv(os.path.join(base_bone_dir, 'boneage-training-dataset.csv'))\nage_df['path'] = age_df['id'].map(lambda x: os.path.join(base_bone_dir,\n                                                         'boneage-training-dataset', \n                                                         'boneage-training-dataset', \n                                                         '{}.png'.format(x)))\nage_df['exists'] = age_df['path'].map(os.path.exists)\nprint(age_df['exists'].sum(), 'images found of', age_df.shape[0], 'total')\nage_df['gender'] = age_df['male'].map(lambda x: 'male' if x else 'female')\nboneage_mean = age_df['boneage'].mean()\nboneage_div = 2*age_df['boneage'].std()\n# we don't want normalization for now\nboneage_mean = 0\nboneage_div = 1.0\nage_df['boneage_zscore'] = age_df['boneage'].map(lambda x: (x-boneage_mean)/boneage_div)\nage_df.dropna(inplace = True)\nage_df.sample(3)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"818da6ca-bbff-4ca0-ad57-ef3a145ae863","_uuid":"688e4340238e013b8459b6f6470993c7de492d83"},"cell_type":"markdown","source":"# Examine the distribution of age and gender\nAge is shown in months"},{"metadata":{"_cell_guid":"5c8bd288-8261-4cbe-a954-e62ac795cc3e","_uuid":"60a8111c4093ca6f69d27a4499442ba7dd750839","trusted":true},"cell_type":"code","source":"age_df[['boneage', 'male', 'boneage_zscore']].hist(figsize = (10, 5))\nage_df['boneage_category'] = pd.cut(age_df['boneage'], 10)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"0ba697ed-85bb-4e9a-9765-4c367db078d1","_uuid":"4df45776bae0b8a1bf9d3eb4eaaebce6e24d726d"},"cell_type":"markdown","source":"# Split Data into Training and Validation"},{"metadata":{"_cell_guid":"1192c6b3-a940-4fa0-a498-d7e0d400a796","_uuid":"a48b300ca4d37a6e8b39f82e3c172739635e4baa","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nraw_train_df, valid_df = train_test_split(age_df, \n                                   test_size = 0.25, \n                                   random_state = 2018,\n                                   stratify = age_df['boneage_category'])\nprint('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"f8060459-da1e-4293-8f61-c7f99de1de9f","_uuid":"26e566d6cec5bd41f9afe392f456ddf7ceb306ea"},"cell_type":"markdown","source":"# Balance the distribution in the training set"},{"metadata":{"_cell_guid":"7a130199-fbf6-4c60-95f5-0797b2f3eaf1","_uuid":"ba7befa238b8c11f9672e3539ac58f3da6955bd9","trusted":true},"cell_type":"code","source":"train_df = raw_train_df.groupby(['boneage_category', 'male']).apply(lambda x: x.sample(500, replace = True)\n                                                      ).reset_index(drop = True)\nprint('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\ntrain_df[['boneage', 'male']].hist(figsize = (10, 5))","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"9954bfda-29bd-4c4d-b526-0a972b3e43e2","_uuid":"9529ab766763a9f122786464c24ab1ebe22c6006","trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\nIMG_SIZE = (384, 384) # slightly smaller than vgg16 normally expects\ncore_idg = ImageDataGenerator(samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range = 0.15, \n                              width_shift_range = 0.15, \n                              rotation_range = 5, \n                              shear_range = 0.01,\n                              fill_mode = 'nearest',\n                              zoom_range=0.25,\n                             preprocessing_function = preprocess_input)","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b5767f42-da63-4737-8f50-749c1a25aa84","_uuid":"07851e798db3d89ba13db7d4b56ab2b759221464","trusted":true},"cell_type":"code","source":"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n    base_dir = os.path.dirname(in_df[path_col].values[0])\n    print('## Ignore next message from keras, values are replaced anyways')\n    df_gen = img_data_gen.flow_from_directory(base_dir, \n                                     class_mode = 'sparse',\n                                    **dflow_args)\n    df_gen.filenames = in_df[path_col].values\n    df_gen.classes = np.stack(in_df[y_col].values)\n    df_gen.samples = in_df.shape[0]\n    df_gen.n = in_df.shape[0]\n    df_gen._set_index_array()\n    df_gen.directory = '' # since we have the full path\n    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n    return df_gen","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"810bd229-fec9-43c4-b3bd-afd62e3e9552","_uuid":"1848f5048a9e00668c3778a85deea97f980e4f1c","trusted":true},"cell_type":"code","source":"train_gen = flow_from_dataframe(core_idg, train_df, \n                             path_col = 'path',\n                            y_col = 'boneage_zscore', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 32)\n\nvalid_gen = flow_from_dataframe(core_idg, valid_df, \n                             path_col = 'path',\n                            y_col = 'boneage_zscore', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 256) # we can use much larger batches for evaluation\n# used a fixed dataset for evaluating the algorithm\ntest_X, test_Y = next(flow_from_dataframe(core_idg, \n                               valid_df, \n                             path_col = 'path',\n                            y_col = 'boneage_zscore', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 1024)) # one big batch","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"2d62234f-aeb0-4eba-8a38-d713d819abf6","_uuid":"8190b4ad60d49fa65af074dd138a19cb8787e983","trusted":true},"cell_type":"code","source":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -127, vmax = 127)\n    c_ax.set_title('%2.0f months' % (c_y*boneage_div+boneage_mean))\n    c_ax.axis('off')","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"da22790a-672c-474e-b118-9eef15b53160","_uuid":"55d665e1e8a8d83b9db005a66a965f8a90c62da1"},"cell_type":"markdown","source":"# Attention Model\nThe basic idea is that a Global Average Pooling is too simplistic since some of the regions are more relevant than others. So we build an attention mechanism to turn pixels in the GAP on an off before the pooling and then rescale (Lambda layer) the results based on the number of pixels. The model could be seen as a sort of 'global weighted average' pooling. There is probably something published about it and it is very similar to the kind of attention models used in NLP.\nIt is largely based on the insight that the winning solution annotated and trained a UNET model to segmenting the hand and transforming it. This seems very tedious if we could just learn attention."},{"metadata":{"_cell_guid":"eeb36110-0cde-4450-a43c-b8f707adb235","_uuid":"1f0dfaccda346d7bc4758e7329d61028d254a8d6","trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\nfrom keras.models import Model\nin_lay = Input(t_x.shape[1:])\nbase_pretrained_model = VGG16(input_shape =  t_x.shape[1:], include_top = False, weights = 'imagenet')\nbase_pretrained_model.trainable = False\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\npt_features = base_pretrained_model(in_lay)\nfrom keras.layers import BatchNormalization\nbn_features = BatchNormalization()(pt_features)\n\n# here we do an attention mechanism to turn pixels in the GAP on an off\n\nattn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)\nattn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\nattn_layer = LocallyConnected2D(1, \n                                kernel_size = (1,1), \n                                padding = 'valid', \n                                activation = 'sigmoid')(attn_layer)\n# fan it out to all of the channels\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n               activation = 'linear', use_bias = False, weights = [up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer)\n\nmask_features = multiply([attn_layer, bn_features])\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\n# to account for missing values from the attention model\ngap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\ngap_dr = Dropout(0.5)(gap)\ndr_steps = Dropout(0.25)(Dense(1024, activation = 'elu')(gap_dr))\nout_layer = Dense(1, activation = 'linear')(dr_steps) # linear is what 16bit did\nbone_age_model = Model(inputs = [in_lay], outputs = [out_layer])\nfrom keras.metrics import mean_absolute_error\ndef mae_months(in_gt, in_pred):\n    return mean_absolute_error(boneage_div*in_gt, boneage_div*in_pred)\n\nbone_age_model.compile(optimizer = 'adam', loss = 'mse',\n                           metrics = [mae_months])\n\nbone_age_model.summary()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"17803ae1-bed8-41a4-9a2c-e66287a24830","_uuid":"48b9764e16fb5af52aed35c82bae6299e67d5bc7","trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('bone_age')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=5) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58a75586-442b-4804-84a6-d63d5a42ea14","_uuid":"b2148479bfe41c5d9fd0faece4c75adea509dabe","trusted":true},"cell_type":"code","source":"bone_age_model.fit_generator(train_gen, \n                                  validation_data = (test_X, test_Y), \n                                  epochs = 15, \n                                  callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4d0c45b0-bb23-48d2-83eb-bc3990043e26","_uuid":"3a90f05dd206cd76c72d8c6278ebb93da41ee45f","trusted":true},"cell_type":"code","source":"# load the best version of the model\nbone_age_model.load_weights(weight_path)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"11f33f0a-61eb-488a-b7ea-4bc9d15ba8f9","_uuid":"cca170eb40bc591f89748ede8aa35de4308faaaf"},"cell_type":"markdown","source":"# Show Attention\nDid our attention model learn anything useful?"},{"metadata":{"collapsed":true,"_cell_guid":"e41a063f-35c9-410f-be63-f66b63ff9683","_uuid":"ad5b085d351e79b950bf0c2ddc476799d5b0692f","trusted":true},"cell_type":"code","source":"# get the attention layer since it is the only one with a single output dim\nfor attn_layer in bone_age_model.layers:\n    c_shape = attn_layer.get_output_shape_at(0)\n    if len(c_shape)==4:\n        if c_shape[-1]==1:\n            print(attn_layer)\n            break","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"340eef36-f5b2-4b15-a59f-440061a427eb","_uuid":"00850972ae4298f49ed1838b3fc49c2d8fb07547","trusted":true},"cell_type":"code","source":"import keras.backend as K\nrand_idx = np.random.choice(range(len(test_X)), size = 6)\nattn_func = K.function(inputs = [bone_age_model.get_input_at(0), K.learning_phase()],\n           outputs = [attn_layer.get_output_at(0)]\n          )\nfig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n    cur_img = test_X[c_idx:(c_idx+1)]\n    attn_img = attn_func([cur_img, 0])[0]\n    img_ax.imshow(cur_img[0,:,:,0], cmap = 'bone')\n    attn_ax.imshow(attn_img[0, :, :, 0], cmap = 'viridis', \n                   vmin = 0, vmax = 1, \n                   interpolation = 'lanczos')\n    real_age = boneage_div*test_Y[c_idx]+boneage_mean\n    img_ax.set_title('Hand Image\\nAge:%2.2fY' % (real_age/12))\n    pred_age = boneage_div*bone_age_model.predict(cur_img)+boneage_mean\n    attn_ax.set_title('Attention Map\\nPred:%2.2fY' % (pred_age/12))\nfig.savefig('attention_map.png', dpi = 300)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24796de7-b1e9-4b3b-bcc6-d997aa3e6d16","_uuid":"244bac80d1ea2074e47932e367996e32cbab6a3d"},"cell_type":"markdown","source":"# Evaluate the results\nHere we evaluate the results by loading the best version of the model and seeing how the predictions look on the results. We then visualize spec"},{"metadata":{"collapsed":true,"_cell_guid":"d0edaf00-4b7c-4f65-af0b-e5a03b9b8428","_uuid":"b421b6183b1919a7414482f0b1ac611079e45174","trusted":true},"cell_type":"code","source":"pred_Y = boneage_div*bone_age_model.predict(test_X, batch_size = 32, verbose = True)+boneage_mean\ntest_Y_months = boneage_div*test_Y+boneage_mean","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"15189df2-3fed-495e-9661-97bb2b712dfd","_uuid":"10162e055ca7cd52878a289bab377231787ab732","trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(1,1, figsize = (6,6))\nax1.plot(test_Y_months, pred_Y, 'r.', label = 'predictions')\nax1.plot(test_Y_months, test_Y_months, 'b-', label = 'actual')\nax1.legend()\nax1.set_xlabel('Actual Age (Months)')\nax1.set_ylabel('Predicted Age (Months)')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c34f049f-b032-45bf-9d5e-a756ecc46a82","_uuid":"ba87d0e7c3a77181487b99ca64d13de2aa8a21ee","trusted":true},"cell_type":"code","source":"ord_idx = np.argsort(test_Y)\nord_idx = ord_idx[np.linspace(0, len(ord_idx)-1, 8).astype(int)] # take 8 evenly spaced ones\nfig, m_axs = plt.subplots(4, 2, figsize = (16, 32))\nfor (idx, c_ax) in zip(ord_idx, m_axs.flatten()):\n    c_ax.imshow(test_X[idx, :,:,0], cmap = 'bone')\n    \n    c_ax.set_title('Age: %2.1fY\\nPredicted Age: %2.1fY' % (test_Y_months[idx]/12.0, \n                                                           pred_Y[idx]/12.0))\n    c_ax.axis('off')\nfig.savefig('trained_img_predictions.png', dpi = 300)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"cfd5c357-83fa-42ef-85d7-1d8667bbdb34","_uuid":"87fd08f683ddececdbe634a0fc4291d02d06b958","trusted":true},"cell_type":"code","source":"!rm -rf ~/.keras","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b87ee27b-7d2c-4eda-91a1-be3c0ec0b763","_uuid":"7f84b11610b58967ae5290042e425c296834ab84","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}